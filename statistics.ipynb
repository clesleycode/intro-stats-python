{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Statistics\n",
    "\n",
    "Welcome to Mathematics for Data Scientists! In this tutorial, we'll review introductory statistics and linear algebra needed for a career in data science. This isn't meant to be a comprehensive guide, but rather a great starting point to get you started on your data science career.\n",
    "\n",
    "In this first section, we'll begin by asking ourselves, \"What *is* statistics?\" It's very likely that you've heard of statistics before, whether that be in an article, results for a test grade in school, or pretty much any other context. But to put it formally, statistics is a discipline that uses data to support claims about populations. You'll come to learn that these \"populations\" are what we refer to as \"distributions.\" \n",
    "\n",
    "Most statistical analysis is based on probability, which is why these pieces are usually presented together. More often than not, you'll see courses labeled \"Intro to Probability and Statistics\" rather than separate intro to probability and intro to statistics courses. This is because probability is the study of random events, or the study of how likely it is that some event will happen.\n",
    "\n",
    "\n",
    "## Technology\n",
    "\n",
    "In this tutorial, we'll be using Python to show how different statistical concepts can be done computationally. Specifically, we'll be working with `scipy` and `numpy`, two scientific computing modules in Python. \n",
    "\n",
    "\n",
    "## Descriptive vs Inferential Statistics\n",
    "\n",
    "Generally speaking, statistics is split into two subfields: descriptive and inferential. The difference is subtle, but important. Descriptive statistics refer to the portion of statistics dedicated to summarizing a total population. Inferential Statistics, on the other hand, allows us to make inferences of a population from its *subpopulation*. Unlike descriptive statistics, inferential statistics are never 100% accurate because its calculations are measured without the total population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics\n",
    "\n",
    "Once again, to review, descriptive statistics refers to the statistical tools used to summarize a dataset. One of the first operations often used to get a sense of what a given data looks like is the `mean` operation. \n",
    "\n",
    "\n",
    "## Mean \n",
    "\n",
    "You know what the mean is, you've heard it every time your computer science professor handed your midterms back and announced that the average, or *mean* was a disappointing low of 59. Woops. \n",
    "\n",
    "With that said, the “average” is just one of many summary statistics you might choose to describe the typical value or the **central tendency** of a sample. You can find the formal mathematical definition right below:\n",
    "\n",
    "$$ \\mu = \\frac{1}{n} \\Sigma_i x_i $$\n",
    "\n",
    "Computing the mean isn't a fun task, especially if you have hundreds, even *thousands* of data points to compute the mean for. You definitely don't want to do this by hand, right? \n",
    "\n",
    "Right. In Python, you can either implement your own mean function, or you can use `numpy`. We'll begin with our own implementation so you can get a thorough understanding of how these sorts of functions are implemented. Below, `t` is a list of datapoints, so you can use built-in list functions such as `sum()` and `len()` to compute the total sum and divide by the total number of data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(t):\n",
    "    return(float(sum(t)) / len(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use built-in functions from the numpy module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([1,4,3,2,6,4,4,3,2,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "In the same way that the mean is intended to describe the central tendency, variance is intended to describe the <b>spread</b>. \n",
    "\n",
    "$$ s_N = \\sqrt {\\frac{1}{N}\\sum\\limits_{i = 1}^N {\\left( {x_i - \\bar x} \\right)^2 } } $$\n",
    "\n",
    "The x<sub>i</sub> - &mu; is called the \"deviation from the mean\", making the variance the squared deviation multiplied by 1 over the number of samples. This is why the square root of the variance, &sigma;, is called the <b>standard deviation</b>.\n",
    "\n",
    "Using the mean function we created above, we'll write up a function that calculates the variance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def var(t, mu):\n",
    "    dev2 = [(x - mu)**2 for x in t]\n",
    "    var = mean(dev2)\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, you can use built in functions from numpy instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4\n"
     ]
    }
   ],
   "source": [
    "print(np.var([1,3,3,6,3,2,7,5,9,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "1. Rewrite the variance function, `variance()`, without calling any outside functions!\n",
    "\n",
    "2. Given the following array, what is the mean (call this `mean1`)?  What is the variance (call this `var1`)?\n",
    "\n",
    "``` python\n",
    "[1,1,1,1,1,1,1]\n",
    "```\n",
    "\n",
    "3. Given the following array, what is the mean (call this `mean2`)?  What is the variance (call this `var2`)?\n",
    "\n",
    "``` python\n",
    "[1,1,1,1,1,1,2]\n",
    "```\n",
    "\n",
    "4. Given the following array which contains an outlier, what is the mean (call this `mean3`)?  What is the variance (call this `var3`)?  Notice how the mean and variance vary from challenge problem 2. \n",
    "\n",
    "\n",
    "``` python\n",
    "[1,1,1,1,1,1,20]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions\n",
    "\n",
    "Summary statistics are concise, but dangerous, because they obscure the data. An alternative is to look at the <b>distribution</b> of the data, which describes how often each value appears.\n",
    "\n",
    "\n",
    "#### Histograms\n",
    "\n",
    "The most common representation of a distribution is a histogram, which is a graph that shows the frequency or probability of each value.\n",
    "\n",
    "Let's say we have the following list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = [1,2,2,3,1,2,3,2,1,3,3,3,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the frequencies, we can represent this with a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 3, 2: 4, 3: 6}\n"
     ]
    }
   ],
   "source": [
    "hist = {}\n",
    "for x in t:\n",
    "    hist[x] = hist.get(x,0) + 1\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to convert these frequencies to probabilities, we divide each frequency by n, where n is the size of our original list. This process is called <b>normalization</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.23076923076923078, 2: 0.3076923076923077, 3: 0.46153846153846156}\n"
     ]
    }
   ],
   "source": [
    "n = float(len(t))\n",
    "pmf = {}\n",
    "for x, freq in hist.items():\n",
    "    pmf[x] = freq / n\n",
    "print(pmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This normalized histogram is called a PMF, “probability mass function”, which is a function that maps values to probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode\n",
    "\n",
    "The most common value in a distribution is called the <b>mode</b>.\n",
    "\n",
    "#### Shape\n",
    "\n",
    "The shape just refers to the shape the histogram data forms. Typically, we look for asymetry, or a lack there of.\n",
    "\n",
    "#### Outliers\n",
    "\n",
    "Outliers are values that are far from the central tendency. Outliers might be caused by errors in collecting or processing the data, or they might be correct but unusual measurements. It is always a good idea to check for outliers, and sometimes it is useful and appropriate to discard them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Create two functions as follows: \n",
    "\n",
    "1. A function named `get_freq`:\n",
    "    + This function must take an array of values as input\n",
    "    + Return a dictionary with its frequencies\n",
    "    \n",
    "2. A function named `get_prob`:\n",
    "    + This function must take an array of values and the dictionary returned by `get_freq`\n",
    "    + Return the dictionary with its probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem 1\n",
    "\n",
    "def get_freq(t):\n",
    "    hist = {}\n",
    "    for x in t:\n",
    "        hist[x] = hist.get(x, 0) + 1\n",
    "    return(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem 2\n",
    "\n",
    "def get_prob(t, hist):\n",
    "    n = float(len(t))\n",
    "    pmf = {}\n",
    "    for x, freq in hist.items():\n",
    "        pmf[x] = freq / n\n",
    "    return(pmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Distribution Functions\n",
    "\n",
    "### Percentiles\n",
    "\n",
    "The percentile rank is the fraction of people who scored lower than you (or the same). So if you are “in the 90th percentile,” you did as well as or better than 90% of the people who took the exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "def percentileRank(scores, your_score):\n",
    "    count = 0\n",
    "    for score in scores:\n",
    "        if score <= your_score:\n",
    "            count += 1\n",
    "    percentile_rank = 100.0 * count / len(scores)\n",
    "    return(percentile_rank)\n",
    "\n",
    "print(percentileRank([1,42,53,23,12,3,35,2], 17.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the `scipy` module to retrieve the percentile rank! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "print(stats.percentileofscore([1,42,53,23,12,3,35,2], 17.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these output the 50th percentile since 17.5 is the median!\n",
    "\n",
    "Now, what if we want the reverse? So instead of what percentile a value is, we want to know what value is at a given percentile. In other words, now we want the inputs and outputs to be switched. Luckily, this is available with `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.percentile([1,42,53,23,12,3,35,2], 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code returns the 50th percentile, e.g median, `17.5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDFs\n",
    "\n",
    "The Cumulative Distribution Function (CDF) is the function that maps values to their percentile rank in a distribution.\n",
    "\n",
    "The following function should look familiar - it's almost the same as percentileRank, except that the result is in a probability in the range 0–1 rather than a percentile rank in the range 0–100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cdf(t, x):\n",
    "    count = 0.0\n",
    "    for value in t:\n",
    "        if value <= x:\n",
    "            count += 1.0\n",
    "    prob = count / len(t)\n",
    "    return(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interquartile Range\n",
    "\n",
    "Once you have computed a CDF, it's easy to compute other summary statistics. The median is just the 50th percentile. The 25th and 75th percentiles are often used to check whether a distribution is symmetric, and their difference, which is called the interquartile range, measures the spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Distributions\n",
    "\n",
    "The distributions we have used so far are called empirical distributions because they are based on empirical observations, which are necessarily finite samples. The alternative is a continuous distribution, which is characterized by a CDF that is a continuous function (as opposed to a step function).\n",
    "\n",
    "### Normal Distribution\n",
    "\n",
    "The normal distribution, also called Gaussian, is the most commonly used because it describes so many scenarios. Despite its wide range of applicability, CDFs with the normal distribution are non-trivial compared to other distributions.\n",
    "\n",
    "Unlike the other distributions we will look at, there is no closed-form expression for the normal CDF. Instead, we write it in terms of the error function, erf(x). \n",
    "\n",
    "![alt text](https://imgur.com/oB5DqVA.png \"Logo Title Text 1\")\n",
    "\n",
    "Now, using the scipy module, we can create the CDF for a Normal Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import erf\n",
    "\n",
    "def NormalCdf(x):\n",
    "    return (erf(x / root2) + 1) / 2\n",
    "\n",
    "def NormalCdf(x, mu=0, sigma=1):\n",
    "    return(StandardNormalCdf(float(x - mu) / sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we imported `er` from `scipy.special`. This CDF, when plotted, looks like:\n",
    "\n",
    "![alt text](https://imgur.com/9LH722Q.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Distribution \n",
    "\n",
    "Exponential distributions come up when we look at a series of events and measure the times between events, which are called interarrival times. If the events are equally likely to occur at any time, the distribution of interarrival times tends to look like an exponential distribution.\n",
    "\n",
    "![alt text](https://github.com/lesley2958/stats-programmers/blob/master/exp%20cdf.png?raw=true\"Logo Title Text 1\")\n",
    "\n",
    "Here, &lambda; determines the shape of the distribution. The mean of an exponential distribution is 1/&lambda;, whereas the median is usually ln(2)/&lambda;. This results in a distribution that looks like:\n",
    "\n",
    "![alt text](https://github.com/lesley2958/stats-programmers/blob/master/exp%20cdf%20plot.png?raw=true \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Write a function `exp_cdf()` that takes x and a series of events as input and returns the probability of that x occuring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto Distribution \n",
    "\n",
    "The Pareto Distribution is often used to describe phenomena in the natural and social sciences including sizes of cities and towns, sand particles and meteorites, forest fires and earthquakes.\n",
    "\n",
    "![alt text](https://github.com/lesley2958/stats-programmers/blob/master/pareto%20cdf.png?raw=true \"Logo Title Text 1\")\n",
    "\n",
    "Here, x<sub>m</sub> and &alpha; determine the location and shape of the distribution. Specifically x<sub>m</sub> is the minimum possible value. This ends up looking something like this: \n",
    "\n",
    "![alt text](https://github.com/lesley2958/stats-programmers/blob/master/pareto%20cdf%20plot.png?raw=true \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Write a function `pareto()` that takes the x variable and an alpha value as input. This function should return the percentile rank of inputted x value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Distribution\n",
    "\n",
    "The Poisson distribution can be applied to systems with a large number of possible events, each of which is rare.\n",
    "\n",
    "A discrete random variable `X` is said to have a Poisson distribution with parameter &lambda; > 0, if, for k = 0, 1, 2,..., the probability mass function of X  is given by:\n",
    "\n",
    "![alt text](https://github.com/lesley2958/stats-programmers/blob/master/poisson%20pmf%20.png?raw=true \"Logo Title Text 1\")\n",
    "\n",
    "The positive real number &lambda; is equal to the expected value of X and its variance, so &lambda; = E(X) = Var(X).\n",
    "\n",
    "To calculate poisson distribution we need two variables:\n",
    "\n",
    "1. Poisson random variable (x): Poisson Random Variable is equal to the overall REMAINING LIMIT that needs to be reached\n",
    "\n",
    "2. Average rate of success(rate_of_success): \n",
    " \n",
    "\n",
    "#### Code\n",
    "\n",
    "Scipy is a python library that is used for Analytics, Scientific Computing, and Technical Computing. Using the `stats.poisson` module we can easily compute poisson distribution of a specific problem.\n",
    "\n",
    "Using scipy, we can calculate the poisson distribution as follows: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proabability_reached = float(1-scipy.stats.distributions.poisson.cdf(poisson random variable-1, rate_of_success)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy can also be used to generate a random Poisson distribution. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 1 0 1 1 1 0 0 1 1 2 0 0 1 0 2 1 4 1 0 1 1 1 0 0 0 0 1 1 0 1 0 3 0 0\n",
      " 0 0 1 0 3 0 0 0 1 2 1 2 1 0 0 3 3 1 1 2 2 1 0 1 1 0 0 1 3 3 0 2 0 2 0 0 0\n",
      " 2 1 3 0 2 2 0 1 1 1 2 0 0 0 2 0 0 1 1 0 1 2 2 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# This generates a poisson distribution of size 100 with lambda value 1\n",
    "print(np.random.poisson(1,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Create a function `def poisson_dist()` that meets the following requirements:\n",
    "\n",
    "1. Generate a random poisson distribution of size 1000 and lambda 10 using above method.\n",
    "2. Returns the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Distributions\n",
    "\n",
    "#### Skewness\n",
    "\n",
    "Skewness is a statistic that measures the asymmetry of a distribution. Given a sequence of values, x<sub>i</sub>, the sample skewness is\n",
    "\n",
    "![alt text](https://github.com/lesley2958/stats-programmers/blob/master/skewness.png?raw=true \"Logo Title Text 1\")\n",
    "\n",
    "You might recognize m<sub>2</sub> as the mean squared deviation (or variance). m<sub>3</sub> is the mean cubed deviation.\n",
    "\n",
    "Negative skewness indicates that a distribution “skews left\". It extends farther to the left than the right. Positive skewness indicates that a distribution skews right.\n",
    "\n",
    "To find this value, you can use `scipy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.592927061281571\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(scipy.stats.skew([1,3,3,6,3,2,7,5,9,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which gets us a measure of `0.592927061281571`, meaning it's skewed to the right.\n",
    "\n",
    "Because outliers can have a disproportionate effect on g<sub>1</sub>, another way to evaluate the asymmetry of a distribution is to look at the relationship between the mean and median. Extreme values have more effect on the mean than the median, so in a distribution that skews left, the mean is less than the median.\n",
    "\n",
    "Take the example from above:\n",
    "\n",
    "```\n",
    "[1,3,3,6,3,2,7,5,9,1]\n",
    "```\n",
    "\n",
    "The median of this list is `3`, whereas the mean is `4`. With these two values, you can confirm that it skews to the right. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Recall the list that we calculated percentiles on, `[1,42,53,23,12,3,35,2]`. Using both methodologies overviewed above, prove that this list of values is `right` skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as sc\n",
    "#import skew_module.skew as sk\n",
    "\n",
    "dis = [1,3,3,6,3,2,7,5,9,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson’s Median Skewness Coefficient\n",
    "\n",
    "Pearson’s median skewness coefficient is an alternative measure of skewness that explicitly captures the relationship between the mean, &mu;, and the median, &mu;<sub>1/2</sub>. It's particularly useful because it's robust, meaning it is <b>not</b> sensitive to outliers.\n",
    "\n",
    "The equation is as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P = (3 * (X - Med))/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where X is the mean, Med is the median, and s is the standard deviation. \n",
    "\n",
    "For `[1,3,3,6,3,2,7,5,9,1]`, the mean is 21, the median is 17.5, and the standard deviation is `18.808`. If we plug these values in, we can a pearson median coefficient of `0.5582781958205234`, meaning it's right skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Compute the pearson median coefficient for `[1,42,53,23,12,3,35,2]` in Python. Your function should be named `pearson_coeff` and should output a float indicated the skewness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pearson_coeff(given):\n",
    "  x = np.mean(given)\n",
    "  med = np.median(given)\n",
    "  s = np.std(given)\n",
    "  # Pearson Median Skewness Coefficient\n",
    "  p = (3*(x-med))/s\n",
    "  return(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Error of Mean\n",
    "\n",
    "In sampling distributions, an estimate is not always equal to its expected value. To determine what the uncertainty of this is, we determine the standard deviation of the distribution of the estimator.\n",
    "\n",
    "Given the mean estimator $\\overline X$ for the sample $X_1, \\ldots, X_n$., the Central Limit Theorem tells us that as $n$ increases, the estimate of $\\overline X$ starts to resemble the normal distribution:\n",
    "\n",
    "$$ \\overline X \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right) $$\n",
    "\n",
    "The **z-score** measures the number of standard deviations the observation is above the mean.  For $\\overline X$, the z-score is distributed as a standard normal with a mean of zero and standard deviation of one.\n",
    "\n",
    "$$ z = \\frac{\\overline X - \\mu}{\\sigma / \\sqrt{n}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student T Distribution\n",
    "\n",
    "Since $\\sigma$ is often unknown, we use the square root of the estimate for the variance $\\sigma^2$ to get$\\hat \\sigma$. This makes the distribution of the z score a Student T distribution instead of a normal one.\n",
    "\n",
    "$$ z = \\frac{\\overline X - \\mu}{\\hat\\sigma / \\sqrt{n}} $$\n",
    "\n",
    "The pdf for this distribution is as follows: \n",
    "\n",
    "$$ p(x) = \\frac{\\Gamma(\\frac{\\nu+1}{2})} {\\sqrt{\\nu\\pi}\\,\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{x^2}{\\nu} \\right)^{\\!-\\frac{\\nu+1}{2}} \\,$$\n",
    "\n",
    "where the mean is $\\mathbb{E}[X] = 0$ and the variance is $\\mbox{Var}[X] = \\frac{\\nu}{\\nu-2}$. It's important to note, however, that as $\\nu \\to \\infty$ (or $n \\to \\infty$), the distribution approaches the standard normal distribution\n",
    "\n",
    "$$ \\overline X \\longrightarrow N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\, $$\n",
    "\n",
    "This results in a standard error of:\n",
    "\n",
    "$$ s = \\frac{\\sigma}{\\sqrt{n}}\\, $$\n",
    "\n",
    "In the example below, we'll take the mean estimate of an exponential random variable. First we initialize n to 1,000 and generate the exponential distribution using `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "expon = scipy.stats.expon(loc=-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_sizes = (None, 10, 100, 1000)\n",
    "mean, var = expon.stats(\"mv\")\n",
    "std = np.sqrt(var)\n",
    "\n",
    "for k, sample_size in enumerate(sample_sizes):\n",
    "    plt.subplot(2,2,k+1)\n",
    "    if sample_size:\n",
    "        rvs = expon.rvs(size=[sample_size, N]).mean(axis=0)\n",
    "        dist = sp.stats.norm(loc=0., scale=1./np.sqrt(sample_size))\n",
    "        plot_hist_dist(\n",
    "            rvs,\n",
    "            dist,\n",
    "            mean=mean,\n",
    "            confidence_interval=[mean - np.sqrt(var / sample_size), mean + np.sqrt(var / sample_size)],\n",
    "            title='mean estimate with {:d}'.format(sample_size),\n",
    "        )\n",
    "    else:\n",
    "        plot_hist_dist(expon.rvs(size=N), expon, title='exponential')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Error of Variance Estimate\n",
    "\n",
    "For normally distributed data, the variance estimator forms a chi-squared distribution. The $\\chi^2$ distribution with $n$ degrees of freedom is the distribution given by the sum of $n$ independent standard normals squared, or: \n",
    "\n",
    "$$ \\chi^2(n) \\sim \\sum_{k=1}^n Z_k^2\\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "norm = scipy.stats.norm()\n",
    "sample_sizes = (None, 10, 100, 1000)\n",
    "\n",
    "for k, sample_size in enumerate(sample_sizes):\n",
    "    plt.subplot(2,2,k+1)\n",
    "    if sample_size:\n",
    "        dist = scipy.stats.chi2(df=sample_size, scale=1./sample_size)\n",
    "        mean, var = dist.stats(\"mv\")        \n",
    "        plot_hist_dist(\n",
    "            sp.stats.norm().rvs(size=[sample_size, N]).var(axis=0),\n",
    "            dist,\n",
    "            mean=mean,\n",
    "            confidence_interval=[mean - np.sqrt(var), mean + np.sqrt(var)],\n",
    "            title='var est with {:d}'.format(sample_size),\n",
    "        )\n",
    "    else:\n",
    "        plot_hist_dist(norm.rvs(size=N), norm, title='normal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability\n",
    "\n",
    "Probability is a real value between 0 and 1 that is intended to be a quantitative measure corresponding to the qualitative notion that some things are more likely than others.\n",
    "\n",
    "The “things” we assign probabilities to are <b>called events</b>. If E represents an event, then P(E) represents the probability that E will occur. A situation where E might or might not happen is called a trial.\n",
    "\n",
    "### Probability Rules\n",
    "\n",
    "Generally speaking, P(A and B) = P(A) P(B), but this is not always true. \n",
    "\n",
    "If two events are mutually exclusive, that means that only one of them can happen, so the conditional probabilities are 0: P(A|B) = P(B|A) = 0. In this case it is easy to compute the probability of either event:\n",
    "P(A or B) = P(A) + P(B)\n",
    "\n",
    "### Binomial Distribution \n",
    "\n",
    "If I roll 100 dice, the chance of getting all sixes is (1/6)<sup>100</sup>. And the chance of getting no sixes is (5/6)<sup>100</sup>. Those cases are easy, but more generally, we might like to know the chance of getting k sixes, for all values of k from 0 to 100. The answer is the binomial distribution, which has this PMF:\n",
    "\n",
    "![alt text](https://github.com/lesley2958/stats-programmers/blob/master/binomial%20pmf.png?raw=true \"Logo Title Text 1\")\n",
    "\n",
    "where n is the number of trials, p is the probability of success, and k is the number of successes. The binomial coefficient is pronounced “n choose k”, and it can be computed\n",
    "recursively like this:\n",
    "\n",
    "![alt text](https://github.com/lesley2958/stats-programmers/blob/master/binomial%20coeff.png?raw=true \"Logo Title Text 1\")\n",
    "\n",
    "In Python, this looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Binom(n, k, d={}):\n",
    "    if k == 0:\n",
    "        return(1)\n",
    "    if n == 0:\n",
    "        return(0)\n",
    "    try:\n",
    "        return(d[n, k])\n",
    "    except KeyError:\n",
    "        res = Binom(n-1, k) + Binom(n-1, k-1)\n",
    "        d[n, k] = res\n",
    "        return(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes's Theorem\n",
    "\n",
    "Bayes’s theorem is a relationship between the conditional probabilities of two events. A conditional probability, often written P(A|B) is the probability that Event A will occur given that we know that Event B has occurred. It's represented as follows:\n",
    "\n",
    "![alt text](https://imgur.com/bY5pxvx.png \"Logo Title Text 1\")\n",
    "\n",
    "Bayes theorem is what allows us to go from a sampling distribution and a prior distribution to a posterior distribution. \n",
    "\n",
    "#### What is a Sampling Distribution?\n",
    "\n",
    "A sampling distribution is the probability of seeing a given data point, given our parameters (&theta;). This is written as p(X|&theta;). For example, we might have data on 1,000 coin flips, where 1 indicates a head.\n",
    "\n",
    "In python, this might look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.495\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_coin_flips = np.random.randint(2, size=1000)\n",
    "print(np.mean(data_coin_flips))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said in the previous section, a sampling distribution allows us to specify how we think these data were generated. For our coin flips, we can think of our data as being generated from a Bernoulli Distribution. \n",
    "\n",
    "Therefore, we can create samples from this distribution like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49\n"
     ]
    }
   ],
   "source": [
    "bernoulli_flips = np.random.binomial(n=1, p=.5, size=1000)\n",
    "print(np.mean(bernoulli_flips))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined how we believe our data were generated, we can calculate the probability of seeing our data given our parameters. Since we have selected a Bernoulli distribution, we only have one parameter, p. \n",
    "\n",
    "We can use the PMF of the Bernoulli distribution to get our desired probability for a single coin flip. Recall that the PMF takes a single observed data point and then given the parameters (p in our case) returns the probablility of seeing that data point given those parameters. \n",
    "\n",
    "For a Bernoulli distribution it is simple: if the data point is a 1, the PMF returns p. If the data point is a 0, it returns (1-p). We could write a quick function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bern_pmf(x, p):\n",
    "    if x == 1:\n",
    "        return(p)\n",
    "    elif x == 0:\n",
    "        return(1 – p)\n",
    "    else:\n",
    "        return(\"Value Not in Support of Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this function to get the probability of a data point give our parameters. You probably see that with p = .5 this function always returns .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(bern_pmf(1, .5))\n",
    "print(bern_pmf(0, .5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More simply, we can also use the built-in methods from scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as st\n",
    "print(st.bernoulli.pmf(1, .3))\n",
    "print(st.bernoulli.pmf(0, .3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation \n",
    "\n",
    "Up until now we have used the symbol &mu; for both the sample mean and the mean parameter, but now we will distinguish them, using x&#772; for the sample mean. Previously, we've just assumed that x&#772; = &mu;, but now we will go through the actual process of estimating &mu;. This process is called estimation, and the statistic we used (the sample mean) is called an estimator.\n",
    "\n",
    "### Outliers\n",
    "\n",
    "Using the sample mean to estimate &mu; is fairly intuitive, but suppose we introduce outliers. One option is to identify and discard outliers, then compute the sample mean of the rest. Another option is to use the median as an estimator.\n",
    "\n",
    "### Mean Squared Error\n",
    "\n",
    "If there are no outliers, the sample mean minimizes the mean squared error (MSE). If we iterate through a dataset, and each time compute the error x&#772; - &mu;, the sample mean minimizes: \n",
    "\n",
    "![alt text](https://imgur.com/PZoIbhQ.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge \n",
    "\n",
    "Write a function `mse()` that takes two lists of numberical datapoints as parameters. The first list should be the set of points of a <i>sample</i> set, while the second should be the set of points from the origin distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "A statistical hypothesis is a hypothesis that is testable on the basis of observing a process that is modeled via a set of random variables. The underlying logic is similar to a proof by contradiction. To prove a mathematical statement, A, you assume temporarily that A is false. If that assumption leads to a contradiction, you conclude that A must actually be true.\n",
    "\n",
    "Similarly, to test a hypothesis like, “This effect is real,” we assume, temporarily, that is is not. That’s the <b>null hypothesis</b>, which is what you typically want to disprove. Based on that assumption, we compute the probability of the apparent effect. That’s the <b>p-value</b>. If the p-value is low enough, we conclude that the null hypothesis is unlikely to\n",
    "be true.\n",
    "\n",
    "### Z-Values, P-Values & Tables\n",
    "\n",
    "These are associated with standard normal distributions. Z-values are a measure of how many standard deviations away from mean is the observed value. P-values are the probabilities, which you can retrieve from its associated z-value in a [z-table](http://www.stat.ufl.edu/~athienit/Tables/Ztable.pdf). \n",
    "\n",
    "We've already reviewed how to retrieve the p-value, but how do we get the z-value? With the following formula:\n",
    "\n",
    "![alt text](https://imgur.com/lAYUX9p.png \"Logo Title Text 1\")\n",
    "\n",
    "where x is your data point, &mu; is the mean and &sigma; is the standard deviation. \n",
    "\n",
    "### Central Limit Theorem\n",
    "\n",
    "The central limit theorem allows us to understand the behavior of estimates across repeated sampling and conclude if a result from a given sample can be declared to be “statistically significant\".\n",
    "\n",
    "The central limit theorem tells us exactly what the shape of the distribution of means will be when we draw repeated samples from a given population.  Specifically, as the sample sizes get larger, the distribution of means calculated from repeated sampling will approach normality. \n",
    "\n",
    "Let's take a look at an example: Here, we have data of 1000 students of 10th standard with their total marks. Let's take a look at the frequency distribution of marks: \n",
    "\n",
    "![alt text](https://imgur.com/9f2k9sF.png \"Logo Title Text 1\")\n",
    "\n",
    "This is clearly an unnatural distribution. So what can we do? \n",
    "\n",
    "Let's take a sample of 40 students from this data. That makes for 25 total samples we can take (1000/40 = 25). The actual mean is 48.4, but it's very unlikely that every sample of 40 will have this same mean. \n",
    "\n",
    "If we take a large number of samples and compute the means and then make a probability histogram on these means, we'll get something similar to:\n",
    "\n",
    "![alt text](https://imgur.com/L8M0MLq.png \"Logo Title Text 1\")\n",
    "\n",
    "You can see that distribution resembles a normally distributed histogram. \n",
    "\n",
    "### Significance Level\n",
    "\n",
    "Significance Tests allow us to see whether there is a significant relationship between variables. It gives us an idea of whether something is likely or unlikely to happen by chance. \n",
    "\n",
    "### Steps\n",
    "\n",
    "The initial step to hypothesis testing is to actually set up the Hypothesis, both the NULL and Alternate.  \n",
    "\n",
    "Next, you set the criteria for decision. To set the criteria for a decision, we state the level of significance for a test. Based on the level of significance, we make a decision to accept the Null or Alternate hypothesis.\n",
    "\n",
    "The third step is to compute the random chance of probability. Higher probability has higher likelihood and enough evidence to accept the Null hypothesis.\n",
    "\n",
    "Lastly, you make a decision. Here, we compare p value with predefined significance level and if it is less than significance level, we reject Null hypothesis, else we accept it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Blood glucose levels for obese patients have a mean of 100 with a standard deviation of 15. A researcher thinks that a diet high in raw cornstarch will have a positive effect on blood glucose levels. A sample of 36 patients who have tried the raw cornstarch diet have a mean glucose level of 108. Test the hypothesis that the raw cornstarch had an effect or not.\n",
    "\n",
    "#### Hypothesis\n",
    "\n",
    "First, we have to state the hypotheses. We set our NULL Hypothesis to be the glucose variable = 100 since that's the known fact. The alternative is that the glucose variable is greater than 100. \n",
    "\n",
    "#### Significance Level\n",
    "\n",
    "Unless specified, we typically set the significance level to 5%, or `0.05`. Now, if we figure out the corresponding z-value from the [z-table](http://www.stat.ufl.edu/~athienit/Tables/Ztable.pdf), we'll see that it corresponds to `1.645`. This is now the z-score cut off for significance level, meaning the area to the right (or z-scores higher than 1.645) is the rejection hypothesis space. \n",
    "\n",
    "#### Computation\n",
    "\n",
    "Now, we can compute the random chance probability using z scores and the z-table. Recall the formula from earlier, z = (x - &mu;)/ &sigma;. Now, before we go into computing, let's overview the difference between standard deviation of the mean and standard deviation of the distribution. \n",
    "\n",
    "When we want to gain a sense the precision of the mean, we calculate what is called the <i>sample distribution of the mean</i>. Assuming statistical independence, the standard deviation of the mean is related to the standard deviation of the distribution with the formula &sigma;<sub>mean</sub> = &sigma / &radic;N. \n",
    "\n",
    "With that knowledge in mind, we've been given the standard deviation of the distribution, but we need the standard deviation of the mean instead. So before we begin calculating the z value, we plug in the values for the formula above. Then we get &sigma;<sub>mean</sub> = 15 / &radic;36, or `2.5`.\n",
    "\n",
    "Now we have all the needed information to compute the z value:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "z = (108-100) / 2.5 = 3.2\n",
    "\n",
    "# .9938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypotheses \n",
    "\n",
    "Awesome! Now we can get the p-value from the z-value above. We see that it corresponds to `.9993`, but we have to remember to subtract this number from 1, making our p-value `0.0007`. Recall that a p-value below 0.05 is grounds for rejecting the null hypothesis. There, we do just that and conclude that there <i>is</i> an effect from the raw starch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "1.  You have a coin and you would like to check whether it is fair or not.  Let `theta` be the probability of heads.  We have two hypothesis.  The null hypothesis is that the could is fair.  What value would we define `theta` to be? -  call this `nulltheta`.  The alternative hypthesis is then the coin is not fair.  (Thus, the alternative hypothetis is not equal to `nulltheta`.)\n",
    "\n",
    "2.  Again let our significance level be 5% or 0.05.  In this case, we are looking at two -tails.  Using the z-table from above, calculate the positive z-value.- call this `zval`.  This means we will accept the null hypothesis for z-scores between -`zval` to +`zval`.  We will accept the alternative hypthosis for z-scores from -infinity to -`zval` or +`zval` to infinity.  \n",
    "\n",
    "3.  To test the null hypthosis and alternative hypothesis, we toss a coin 100 times and record the number of heads.  Calculate the mean. - call this `mean`.  Calculate the the standard deviation.- call this `standev`. (hint: this is a binomial distribution.)\n",
    "\n",
    "4.  Let's look at `coin1`. Let's say we toss one coin, `coin1`, 100 times and we get 45 heads. Calculate this z - score. - call it `zs1`. Does this accept the null hypotheis or the alternative hypothesis.  Label  `coin1` as 'null' or 'alternative'.  \n",
    "\n",
    "5.  Let's look at `coin2`. Let's say we toss another coin, `coin2`, 100 times and we get 61 heads. Calculate this z - value. - call it `zs2`.  Does this accept the null hypotheis or the alternative hypothesis.  Label  `coin2` as 'null' or 'alternative'.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "Now, we'll look at relationships between variables. <b>Correlation</b> is a description of the relationship between two variables.\n",
    "\n",
    "### Covariance\n",
    "\n",
    "Covariance is a measure of the tendency of two variables to vary together. If we have two series, X and Y, their deviations from the mean are\n",
    "\n",
    "![alt text](https://imgur.com/Mou3vhA.png \"Logo Title Text 1\")\n",
    "\n",
    "where &mu;<sub>X</sub> is the mean of X and &mu;<sub>Y</sub> is the mean of Y. If X and Y vary together, their deviations tend to have the same sign. If we multiply them together, the product is positive when the deviations have the same sign and negative when they have the opposite sign. So adding up the products gives a measure of the tendency to vary together.\n",
    "\n",
    "Therefore, covariance is the mean of these two products:\n",
    "\n",
    "![alt text](https://imgur.com/oYarfkU.png \"Logo Title Text 1\")\n",
    "\n",
    "Note that n is the length of the two series, so they have to be the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Cov(xs, ys, mux=None, muy=None):\n",
    "    \"\"\"Computes Cov(X, Y).\n",
    "\n",
    "    Args:\n",
    "        xs: sequence of values\n",
    "        ys: sequence of values\n",
    "        mux: optional float mean of xs\n",
    "        muy: optional float mean of ys\n",
    "\n",
    "    Returns:\n",
    "        Cov(X, Y)\n",
    "    \"\"\"\n",
    "    if mux is None:\n",
    "        mux = thinkstats.Mean(xs)\n",
    "    if muy is None:\n",
    "        muy = thinkstats.Mean(ys)\n",
    "\n",
    "    total = 0.0\n",
    "    for x, y in zip(xs, ys):\n",
    "        total += (x-mux) * (y-muy)\n",
    "\n",
    "    return(total / len(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "One solution to this problem is to divide the deviations by &sigma;, which yields standard scores, and compute the product of standard scores.\n",
    "\n",
    "![alt text](https://imgur.com/VhIu2go.png \"Logo Title Text 1\")\n",
    "\n",
    "Pearson’s correlation is always between -1 and +1. The magnitude indicates the strength of the correlation. If p = 1 the variables are perfectly correlated. The same is true if p = -1. It means that the variables are negatively correlated.\n",
    "\n",
    "It's important to note that Pearson's correlation only measures <b>linear</b> relationships. \n",
    "\n",
    "Using the mean, variances, and covariance methods above, we can write a function that calculates the correlation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def Corr(xs, ys):\n",
    "    xbar = Mean(xs)\n",
    "    varx = Var(xs)\n",
    "    ybar = Mean(ys)\n",
    "    vary = Var(ys)\n",
    "\n",
    "    corr = Cov(xs, ys, xbar, ybar) / math.sqrt(varx * vary)\n",
    "    return(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Intervals\n",
    "\n",
    "The formal meaning of a confidence interval is that 95% of the confidence intervals should, in the long run, contain the true population parameter. Mathematically, this is defined as $z$-$\\sigma$  or $z$-standard-deviation **confidence interval** (where $z \\ge 0$) to be the interval\n",
    "\n",
    "$$ [\\overline X - zs, \\overline X + zs] \\,.$$\n",
    "\n",
    "Assuming the mean estimate is normally distributed, then we can use the statistics of the normal distribution to compute the probability that the mean estimate falls within the $z$-$\\sigma$ confidence interval.  If $n(x\\mid\\mu,\\sigma)$ is the normal pdf with mean $\\mu$ and standard deviation $\\sigma$, then the probability the mean falls within the $z$-$\\sigma$ confidence interval is\n",
    "\n",
    "$$ \\int_{\\overline X - zs}^{\\overline X + zs} n(x \\mid \\overline X, s)dx = \\int_{- z}^{z} n(x \\mid 0, 1)dx = N(z) - N(-z) $$\n",
    "\n",
    "where $N$ is the cumulative normal distribution.  We usually choose $z$ to be 2, indicating the ~95% confidence interval, or 3, indicating the ~99% confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "norm = scipy.stats.norm()\n",
    "\n",
    "zs = (1, 2, 2.5, 3)\n",
    "\n",
    "for k, z in enumerate(zs):\n",
    "    plt.subplot(2,2,k+1)\n",
    "    plot_hist_dist(norm.rvs(size=N), norm)\n",
    "    plt.plot([-z, -z], [0, norm.pdf(-z)], 'r', label='CI')\n",
    "    plt.plot([z, z], [0, norm.pdf(z)], 'r')\n",
    "    prob = norm.cdf(z) - norm.cdf(-z)\n",
    "    plt.title('{} sigma CI: {:.4f}'.format(z, prob))\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure()\n",
    "z=np.arange(0, 3., .1-1e-9)\n",
    "plt.plot(z, norm.cdf(z) - norm.cdf(-z))\n",
    "plt.xlabel('z score')\n",
    "plt.ylabel('probability captured in CI')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
